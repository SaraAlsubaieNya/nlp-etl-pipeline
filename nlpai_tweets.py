# -*- coding: utf-8 -*-
"""NLPAI_tweets.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14hagVB7Xzz2ISDidqDAH-ctVnKr7qG_z

# ETL + NLP
"""

import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('vader_lexicon')

#!pip install vaderSentiment

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import re
from textblob import TextBlob
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from transformers import pipeline
from nltk.corpus import stopwords
from wordcloud import WordCloud

plt.style.use('default')
sns.set_palette("husl")

filename = 'Final Tweets.csv'
print(f"Loading dataset from: {filename}")

df = pd.read_csv(filename)

df = pd.read_csv(filename)

print(f"Dataset shape: {df.shape}")
print(f"Columns: {list(df.columns)}")
print("\nFirst few rows:")
print(df.head())
print("\nMissing values:")
print(df.isnull().sum())

def clean_text(text):
    """Clean tweet text"""
    if pd.isna(text):
        return ""

    text = str(text).lower()
   #cleaning here
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    text = re.sub(r'@\w+|#', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    text = re.sub(r'[^a-zA-Z\s]', '', text)

    return text

def get_text_features(text):
    """Extract text features"""
    if pd.isna(text):
        return pd.Series([0, 0, 0, 0, 0])

    text = str(text)
    return pd.Series([
        len(text.split()),  # word_count
        len(text),          # char_count
        text.count('#'),    # hashtag_count
        text.count('@'),    # mention_count
        len(re.findall(r'http\S+|www\S+|https\S+', text))  # url_count
    ])

print("Cleaning functions defined!")

text_columns = ['Serial', 'Tweets', 'Sentiment']
text_col = None

for col in text_columns:
    if col in df.columns:
        text_col = col
        break

if text_col is None:
    print("Available columns:", list(df.columns))
    text_col = input("Enter the name of your text column: ")

print(f"Using column: '{text_col}'")


df['cleaned_text'] = df[text_col].apply(clean_text)

feature_cols = ['word_count', 'char_count', 'hashtag_count', 'mention_count', 'url_count']
df[feature_cols] = df[text_col].apply(get_text_features)

print("Text cleaning completed!")
print(f"Sample cleaned text:\n{df['cleaned_text'].iloc[0]}")

#i fixed the dataset here
text_col = 'Tweets'
print(f"Using column: '{text_col}'")

df['cleaned_text'] = df[text_col].apply(clean_text)

feature_cols = ['word_count', 'char_count', 'hashtag_count', 'mention_count', 'url_count']
df[feature_cols] = df[text_col].apply(get_text_features)

print("Text cleaning completed with correct column!")
print(f"\nSample original tweet:\n{df[text_col].iloc[0]}")
print(f"\nSample cleaned text:\n{df['cleaned_text'].iloc[0]}")


if 'Sentiment' in df.columns:
    print(f"\nOriginal sentiment distribution:")
    print(df['Sentiment'].value_counts())

vader_analyzer = SentimentIntensityAnalyzer()

#transformer model
print("Loading transformer model...")
try:
    transformer_sentiment = pipeline(
        "sentiment-analysis",
        model="cardiffnlp/twitter-roberta-base-sentiment-latest",
        return_all_scores=True
    )
    print("Transformer model loaded!")
except:
#Fallback to simpler model
    transformer_sentiment = pipeline("sentiment-analysis")
    print("Fallback sentiment model loaded!")

def analyze_sentiment(text):
    """Analyze sentiment using multiple methods"""
    if pd.isna(text) or text == "":
        return pd.Series(['neutral', 0, 0, 'neutral', 0])

    #TextBlob
    blob = TextBlob(text)
    tb_polarity = blob.sentiment.polarity
    tb_sentiment = 'positive' if tb_polarity > 0.1 else 'negative' if tb_polarity < -0.1 else 'neutral'

    #VADER
    vader_scores = vader_analyzer.polarity_scores(text)
    vader_compound = vader_scores['compound']
    vader_sentiment = 'positive' if vader_compound > 0.05 else 'negative' if vader_compound < -0.05 else 'neutral'

    #Transformer
    try:
        #if the text is too long
        text_truncated = text[:512] if len(text) > 512 else text
        result = transformer_sentiment(text_truncated)

        if isinstance(result[0], list):

            best_result = max(result[0], key=lambda x: x['score'])
        else:

            best_result = result[0]

        label = best_result['label'].lower()
        score = best_result['score']


        label_mapping = {
            'label_0': 'negative', 'label_1': 'neutral', 'label_2': 'positive',
            'negative': 'negative', 'neutral': 'neutral', 'positive': 'positive'
        }
        final_sentiment = label_mapping.get(label, label)

    except Exception as e:
        print(f"Transformer error for text: {text[:50]}... Error: {str(e)}")
        final_sentiment = vader_sentiment
        score = abs(vader_compound)

    return pd.Series([tb_sentiment, tb_polarity, vader_compound, final_sentiment, score])

print("Sentiment analysis functions ready!")

print("Running sentiment analysis")


batch_size = 100
n_batches = len(df) // batch_size + 1

sentiment_results = []

for i in range(n_batches):
    start_idx = i * batch_size
    end_idx = min((i + 1) * batch_size, len(df))

    if start_idx >= len(df):
        break

    batch_texts = df['cleaned_text'].iloc[start_idx:end_idx]
    batch_results = batch_texts.apply(analyze_sentiment)
    sentiment_results.append(batch_results)

    if (i + 1) % 10 == 0:
        print(f"Processed {end_idx} / {len(df)} tweets...")

#Combining results
sentiment_df = pd.concat(sentiment_results, ignore_index=True)
sentiment_df.columns = ['textblob_sentiment', 'textblob_polarity', 'vader_compound', 'final_sentiment', 'confidence_score']

#adding to main dataframe
df = pd.concat([df, sentiment_df], axis=1)
df['processed_at'] = datetime.now()

print("Sentiment analysis completed!")

print("\n SENTIMENT ANALYSIS RESULTS")
print("=" * 50)

total_tweets = len(df)
print(f"Total tweets analyzed: {total_tweets:,}")

sentiment_counts = df['final_sentiment'].value_counts()
print(f"\n Final Sentiment Distribution:")
for sentiment, count in sentiment_counts.items():
    percentage = (count / total_tweets) * 100
    print(f"  {sentiment.title()}: {count:,} ({percentage:.1f}%)")

#Text statistics
print(f"\n Text Statistics:")
print(f"  Average word count: {df['word_count'].mean():.1f}")
print(f"  Average character count: {df['char_count'].mean():.1f}")
print(f"  Tweets with hashtags: {(df['hashtag_count'] > 0).sum():,}")
print(f"  Tweets with mentions: {(df['mention_count'] > 0).sum():,}")

#Method comparison
print(f"\n Method Agreement:")
methods = ['textblob_sentiment', 'final_sentiment']
agreement = (df['textblob_sentiment'] == df['final_sentiment']).mean()
print(f"  TextBlob vs Transformer: {agreement:.1%}")

fig, axes = plt.subplots(2, 3, figsize=(18, 12))
fig.suptitle('AI Tweet Sentiment Analysis Dashboard', fontsize=16, fontweight='bold')

#Sentiment Distribution
sentiment_counts = df['final_sentiment'].value_counts()
colors = ['#ff6b6b', '#4ecdc4', '#45b7d1']
axes[0, 0].pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%',
               colors=colors, startangle=90)
axes[0, 0].set_title('Sentiment Distribution')

#Word Count by Sentiment
for sentiment in sentiment_counts.index:
    data = df[df['final_sentiment'] == sentiment]['word_count']
    axes[0, 1].hist(data, alpha=0.7, label=sentiment.title(), bins=20)
axes[0, 1].set_title('Word Count Distribution by Sentiment')
axes[0, 1].set_xlabel('Word Count')

output_filename = 'processed_ai_tweets_with_sentiment.csv'
df.to_csv(output_filename, index=False)

print("LOAD PHASE COMPLETED!")
print(f"Saved processed data to: {output_filename}")
print(f"Original dataset: {len(df)} rows")
print(f"Added {len([col for col in df.columns if 'sentiment' in col.lower() or col in ['cleaned_text', 'word_count', 'char_count']])} new features!")

#simple showcase
print(f"ETL PIPELINE SUMMARY:")
print(f"EXTRACT: Loaded {len(df):,} AI tweets")
print(f"TRANSFORM: Applied NLP processing + sentiment analysis")
print(f"LOAD: Saved enriched dataset with sentiment predictions")

print(f"\nNew columns added:")
new_cols = ['cleaned_text', 'word_count', 'char_count', 'hashtag_count',
           'mention_count', 'url_count', 'textblob_sentiment', 'textblob_polarity',
           'vader_compound', 'final_sentiment', 'confidence_score', 'processed_at']
for col in new_cols:
    if col in df.columns:
        print(f"   {col}")

"""# DATA PIPELINE ;3"""

import pandas as pd
import numpy as np
from datetime import datetime
import re
import json
import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import time

#NLP Libraries
from textblob import TextBlob
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from transformers import pipeline


class AITweetSentimentPipeline:
    """
    Production-ready data pipeline for AI tweet sentiment analysis
    Supports batch processing, error handling, logging, and monitoring
    """

    def __init__(self, config: Optional[Dict] = None):
        """Initialize the pipeline with configuration"""

        #Default configuration
        self.config = {
            'batch_size': 100,
            'max_text_length': 512,
            'output_format': 'csv',
            'enable_logging': True,
            'log_level': 'INFO',
            'retry_attempts': 3,
            'transformer_model': 'cardiffnlp/twitter-roberta-base-sentiment-latest',
            'output_dir': './pipeline_outputs',
            'enable_validation': True
        }


        if config:
            self.config.update(config)


        Path(self.config['output_dir']).mkdir(exist_ok=True)

        if self.config['enable_logging']:
            self._setup_logging()


        self.sentiment_analyzer = SentimentIntensityAnalyzer()
        self.transformer_model = None
        self.pipeline_stats = {
            'start_time': None,
            'end_time': None,
            'total_records': 0,
            'processed_records': 0,
            'failed_records': 0,
            'processing_time': 0
        }


        self.logger.info("Pipeline initialized successfully")

    def _setup_logging(self):
        """Setup logging configuration"""
        logging.basicConfig(
            level=getattr(logging, self.config['log_level']),
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(f"{self.config['output_dir']}/pipeline.log"),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def _load_transformer_model(self):
        """Lazy loading of transformer model"""
        if self.transformer_model is None:
            try:
                self.logger.info(f"Loading transformer model: {self.config['transformer_model']}")
                self.transformer_model = pipeline(
                    "sentiment-analysis",
                    model=self.config['transformer_model'],
                    return_all_scores=True
                )
                self.logger.info("Transformer model loaded successfully")
            except Exception as e:
                self.logger.warning(f"Failed to load transformer model: {e}")
                self.logger.info("Falling back to basic sentiment pipeline")
                self.transformer_model = pipeline("sentiment-analysis")

    def extract(self, data_source: str, **kwargs) -> pd.DataFrame:
        """
        Extract data from various sources
        Supports: CSV, JSON, Excel, Database connections
        """
        self.logger.info(f"Starting data extraction from: {data_source}")

        try:

            if data_source.endswith('.csv'):
                df = pd.read_csv(data_source, **kwargs)
            elif data_source.endswith('.json'):
                df = pd.read_json(data_source, **kwargs)
            elif data_source.endswith(('.xlsx', '.xls')):
                df = pd.read_excel(data_source, **kwargs)
            else:
                raise ValueError(f"Unsupported file format: {data_source}")

            self.pipeline_stats['total_records'] = len(df)
            self.logger.info(f"Successfully extracted {len(df)} records")


            if self.config['enable_validation']:
                self._validate_input_data(df)

            return df

        except Exception as e:
            self.logger.error(f"Error during extraction: {e}")
            raise

    def _validate_input_data(self, df: pd.DataFrame):
        """Validate input data quality"""
        self.logger.info("Validating input data...")

        #required columns
        text_columns = ['text', 'tweet', 'tweets', 'content', 'message', 'Text', 'Tweet', 'Tweets']
        text_col = None

        for col in text_columns:
            if col in df.columns:
                text_col = col
                break

        if text_col is None:
            raise ValueError(f"No text column found. Available columns: {list(df.columns)}")

        #for the data quality
        null_percentage = df[text_col].isnull().sum() / len(df) * 100
        if null_percentage > 50:
            self.logger.warning(f"High null percentage in text column: {null_percentage:.1f}%")

        self.logger.info(f"Data validation passed. Using text column: {text_col}")
        return text_col

    def transform(self, df: pd.DataFrame, text_column: Optional[str] = None) -> pd.DataFrame:
        """
        Transform data with comprehensive NLP processing
        """
        self.logger.info("Starting data transformation...")
        self.pipeline_stats['start_time'] = datetime.now()


        if text_column is None:
            text_column = self._validate_input_data(df)

        #Create processing copy
        processed_df = df.copy()

        #Add metadata
        processed_df['pipeline_id'] = f"pipeline_{int(time.time())}"
        processed_df['processed_timestamp'] = datetime.now()

        #Text cleaning and feature extraction
        self.logger.info("Applying text preprocessing...")
        processed_df = self._apply_text_processing(processed_df, text_column)

        #Sentiment analysis
        self.logger.info("Performing sentiment analysis...")
        processed_df = self._apply_sentiment_analysis(processed_df)


        processed_df = self._add_quality_metrics(processed_df)

        self.pipeline_stats['end_time'] = datetime.now()
        self.pipeline_stats['processing_time'] = (
            self.pipeline_stats['end_time'] - self.pipeline_stats['start_time']
        ).total_seconds()

        self.logger.info(f"Transformation completed in {self.pipeline_stats['processing_time']:.2f} seconds")
        return processed_df

    def _apply_text_processing(self, df: pd.DataFrame, text_column: str) -> pd.DataFrame:
        """Apply text cleaning and feature extraction"""

        #Text cleaning
        df['cleaned_text'] = df[text_column].apply(self._clean_text)

        #Feature extraction
        features = df[text_column].apply(self._extract_text_features)

        feature_df = pd.DataFrame(list(features))

        return pd.concat([df, feature_df], axis=1)

    def _clean_text(self, text) -> str:
        """Clean individual text entry"""
        if pd.isna(text):
            return ""

        text = str(text).lower()
        #cleaning
        text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
        text = re.sub(r'@\w+|#', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        text = re.sub(r'[^a-zA-Z\s]', '', text)

        return text

    def _extract_text_features(self, text) -> Dict:
        """Extract text features from individual entry"""
        if pd.isna(text):
            return {
                'word_count': 0, 'char_count': 0, 'hashtag_count': 0,
                'mention_count': 0, 'url_count': 0, 'avg_word_length': 0
            }

        text = str(text)
        words = text.split()

        return {
            'word_count': len(words),
            'char_count': len(text),
            'hashtag_count': text.count('#'),
            'mention_count': text.count('@'),
            'url_count': len(re.findall(r'http\S+|www\S+|https\S+', text)),
            'avg_word_length': np.mean([len(word) for word in words]) if words else 0
        }

    def _apply_sentiment_analysis(self, df: pd.DataFrame) -> pd.DataFrame:
        """Apply multiple sentiment analysis methods"""

        #Load transformer model
        self._load_transformer_model()


        batch_size = self.config['batch_size']
        results = []

        for i in range(0, len(df), batch_size):
            batch = df.iloc[i:i+batch_size]
            batch_results = self._process_sentiment_batch(batch)
            results.append(batch_results)

            if (i // batch_size + 1) % 10 == 0:
                self.logger.info(f"Processed {min(i+batch_size, len(df))}/{len(df)} records")

        #Combining results
        sentiment_df = pd.concat(results, ignore_index=True)
        return pd.concat([df, sentiment_df], axis=1)

    def _process_sentiment_batch(self, batch: pd.DataFrame) -> pd.DataFrame:
        """Process sentiment analysis for a batch of texts"""

        results = []
        for _, row in batch.iterrows():
            try:
                sentiment_result = self._analyze_single_sentiment(row['cleaned_text'])
                results.append(sentiment_result)
                self.pipeline_stats['processed_records'] += 1
            except Exception as e:
                self.logger.warning(f"Failed to process record: {e}")

                results.append({
                    'textblob_polarity': 0, 'textblob_sentiment': 'neutral',
                    'vader_compound': 0, 'vader_sentiment': 'neutral',
                    'transformer_sentiment': 'neutral', 'confidence_score': 0,
                    'final_sentiment': 'neutral', 'processing_error': True
                })
                self.pipeline_stats['failed_records'] += 1

        return pd.DataFrame(results)

    def _analyze_single_sentiment(self, text: str) -> Dict:
        """Analyze sentiment for a single text entry"""

        if pd.isna(text) or text == "":
            return {
                'textblob_polarity': 0, 'textblob_sentiment': 'neutral',
                'vader_compound': 0, 'vader_sentiment': 'neutral',
                'transformer_sentiment': 'neutral', 'confidence_score': 0,
                'final_sentiment': 'neutral', 'processing_error': False
            }

        #TextBlob analysis
        blob = TextBlob(text)
        tb_polarity = blob.sentiment.polarity
        tb_sentiment = 'positive' if tb_polarity > 0.1 else 'negative' if tb_polarity < -0.1 else 'neutral'

        #VADER analysis
        vader_scores = self.sentiment_analyzer.polarity_scores(text)
        vader_compound = vader_scores['compound']
        vader_sentiment = 'positive' if vader_compound > 0.05 else 'negative' if vader_compound < -0.05 else 'neutral'

        #Transformer analysis
        try:
            text_truncated = text[:self.config['max_text_length']]
            result = self.transformer_model(text_truncated)

            if isinstance(result[0], list):
                best_result = max(result[0], key=lambda x: x['score'])
            else:
                best_result = result[0]

            label = best_result['label'].lower()
            confidence = best_result['score']

            #mapping labels
            label_mapping = {
                'label_0': 'negative', 'label_1': 'neutral', 'label_2': 'positive',
                'negative': 'negative', 'neutral': 'neutral', 'positive': 'positive'
            }
            transformer_sentiment = label_mapping.get(label, label)

        except Exception as e:
            transformer_sentiment = vader_sentiment
            confidence = abs(vader_compound)

        return {
            'textblob_polarity': tb_polarity,
            'textblob_sentiment': tb_sentiment,
            'vader_compound': vader_compound,
            'vader_sentiment': vader_sentiment,
            'transformer_sentiment': transformer_sentiment,
            'confidence_score': confidence,
            'final_sentiment': transformer_sentiment,  # Use transformer as primary
            'processing_error': False
        }


    def _add_quality_metrics(self, df: pd.DataFrame) -> pd.DataFrame:
        """Add data quality and processing metrics"""

        # Processing quality
        df['text_quality_score'] = (
            (df['word_count'] > 3).astype(int) * 0.3 +  # Has meaningful content
            (df['char_count'] > 10).astype(int) * 0.3 +  # Not too short
            (~df['cleaned_text'].str.contains(r'^\s*$', na=True)).astype(int) * 0.4  # Not empty after cleaning
        )


        df['sentiment_consistency'] = (
            (df['textblob_sentiment'] == df['vader_sentiment']).astype(int) * 0.3 +
            (df['vader_sentiment'] == df['transformer_sentiment']).astype(int) * 0.4 +
            (df['textblob_sentiment'] == df['transformer_sentiment']).astype(int) * 0.3
        )

        return df

    def load(self, df: pd.DataFrame, output_path: Optional[str] = None) -> str:
        """
        Load processed data to various destinations
        """
        self.logger.info("Starting data loading...")


        if output_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"{self.config['output_dir']}/processed_tweets_{timestamp}"

        try:

            if self.config['output_format'] == 'csv':
                full_path = f"{output_path}.csv"
                df.to_csv(full_path, index=False)
            elif self.config['output_format'] == 'json':
                full_path = f"{output_path}.json"
                df.to_json(full_path, orient='records', indent=2)
            elif self.config['output_format'] == 'parquet':
                full_path = f"{output_path}.parquet"
                df.to_parquet(full_path, index=False)
            else:
                raise ValueError(f"Unsupported output format: {self.config['output_format']}")

            #Save pipeline metdata
            self._save_pipeline_metadata(f"{output_path}_metadata.json")

            self.logger.info(f"Data successfully loaded to: {full_path}")
            return full_path

        except Exception as e:
            self.logger.error(f"Error during loading: {e}")
            raise

    def _save_pipeline_metadata(self, metadata_path: str):
        """Save pipeline execution metadata"""

        metadata = {
            'pipeline_config': self.config,
            'execution_stats': self.pipeline_stats,
            'execution_summary': {
                'total_records': self.pipeline_stats['total_records'],
                'successfully_processed': self.pipeline_stats['processed_records'],
                'failed_records': self.pipeline_stats['failed_records'],
                'success_rate': (self.pipeline_stats['processed_records'] /
                               max(self.pipeline_stats['total_records'], 1)) * 100,
                'processing_speed': (self.pipeline_stats['processed_records'] /
                                   max(self.pipeline_stats['processing_time'], 1))
            }
        }

        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2, default=str)

        self.logger.info(f"Pipeline metadata saved to: {metadata_path}")

    def run_pipeline(self, data_source: str, output_path: Optional[str] = None,
                    text_column: Optional[str] = None) -> Tuple[pd.DataFrame, str]:
        """
        Run the complete ETL pipeline
        """
        self.logger.info("🚀 Starting AI Tweet Sentiment Analysis Pipeline")

        try:
            # Extract
            df = self.extract(data_source)

            # Transform
            processed_df = self.transform(df, text_column)

            # Load
            output_file = self.load(processed_df, output_path)

            # Pipeline summary
            self._log_pipeline_summary()

            return processed_df, output_file

        except Exception as e:
            self.logger.error(f"Pipeline failed: {e}")
            raise

    def _log_pipeline_summary(self):
        """Log comprehensive pipeline summary"""

        self.logger.info("=" * 60)
        self.logger.info(" PIPELINE EXECUTION SUMMARY")
        self.logger.info("=" * 60)
        self.logger.info(f"Total Records: {self.pipeline_stats['total_records']:,}")
        self.logger.info(f"Successfully Processed: {self.pipeline_stats['processed_records']:,}")
        self.logger.info(f"Failed Records: {self.pipeline_stats['failed_records']:,}")
        self.logger.info(f"Success Rate: {(self.pipeline_stats['processed_records']/max(self.pipeline_stats['total_records'],1)*100):.1f}%")
        self.logger.info(f"Processing Time: {self.pipeline_stats['processing_time']:.2f} seconds")
        self.logger.info(f"Processing Speed: {(self.pipeline_stats['processed_records']/max(self.pipeline_stats['processing_time'],1)):.1f} records/second")
        self.logger.info(" Pipeline completed successfully!")




sentiment_pipeline_instance = AITweetSentimentPipeline()

processed_df, output_file_path = sentiment_pipeline_instance.run_pipeline(data_source=filename, text_column='Tweets')

# Display results
print("\nPipeline execution finished.")
print(f"Processed data saved to: {output_file_path}")
print("\nProcessed DataFrame Info:")
print(f"Shape: {processed_df.shape}")
print("\nFirst 5 rows of processed data:")
print(processed_df.head())
print("\nValue counts for final sentiment:")
print(processed_df['final_sentiment'].value_counts())
